---
title: "GeoSurvey Workflows"
author: M. G. Walsh
date: "21/11/2020"
output:
  html_document
    css: style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

We gratefully acknowledge the team at [**QED**](https://qed.ai/), who built, maintain and continuously improve the [**GeoSurvey**](https://geosurvey.qed.ai/) App.

[**Dr. Alex Verlinden**](https://www.linkedin.com/in/alex-verlinden-1782a79/) (1956 - 2017) is gratefully acknowledged for his many contributions in developing GeoSurvey applications as well as for thoroughly training dozens of people in their uses. 

## Introduction {#gs-intro}

Quantifying the geographical extent, location and spatial dynamics of cropland areas, rural and urban settlements and woody vegetation cover provides essential land cover information for monitoring and managing human dominated (*"anthropic"*) landscapes and the associated socio-economic, health, environmental and ecological impacts. Large portions of Africa remain *"terra incognita"* in this context. Hence, the main reason for gathering GeoSurvey observations is that these allow us to rapidly assess where in a particular country significant impacts of humans on ecosystem processes can be expected. We have frequently been able to provide highly accurate assessments and spatial predictions of where anthropic impacts are likely to occur in remote places that are not readily reachable nor explorable by car or even on foot.

[**GeoSurvey**](https://geosurvey.qed.ai/) is a platform for analyzing geospatial land cover observations. High resolution satellite images, mobile phone photography, and drone imagery can be systematically labeled by either trained photo interpreters and/or by *"crowds"* of [**Citizen Scientists**](https://en.wikipedia.org/wiki/Citizen_science). When done with care, these observations result in large, well-structured, properly labeled, geospatial data sets that are suitable for machine learning and geostatistical predictions of land cover and in some instances for predicting land use. The detailed manual for conducting your own GeoSurveys is available at: [**GeoSurvey manual**](https://docs.google.com/document/d/1y-HYUSYpDVESPdmEcl3I2kuL0bwrT41wMiq0zE9uzOs/edit). The manual should definitely be consulted to obtain information about how GeoSurvey can be used to carry out potentially high value surveys of remote areas. There is also a great slide deck available [**here**](https://docs.google.com/presentation/d/1vBQ-By8LLvyJQzMBFaqUuRwFFeL7Y8QXUtBifx-3jn4/edit#slide=id.g14d47405c8_0_0), which illustrates different land cover and use labeling approaches. We shall not cover these issues in this chapter and assume that we already have well-designed GeoSurvey data and collocated spatial covariates in hand.

```{r GS_screenshot, echo=FALSE, fig.cap="Screenshot of the [**GeoSurvey App**](https://geosurvey.qed.ai/) in action over a quadrat in Rwanda", fig.align="center", out.width = '90%'}
knitr::include_graphics("figures/geosurvey_screenshot.png")
```

This notebook provides an overview of ensemble machine learning and statistical workflows, which we have used for mapping and interpreting various GeoSurvey land cover observations from Digital Globe (Worldview etc) satellite images. We recognize that there are potentially many different techniques and algorithms that could be applied in this context. Hence, the workflows presented here are completely open for constructive criticism and improvement. They could (and probably should) also be exposed and stress-tested in data science competitions e.g., at [**Kaggle**](https://www.kaggle.com/), given their potential importance for monitoring and managing terra incognita landscapes.

The intent of the chapter is to describe starter code for predictive mapping and statistical small area estimation [**SAE**](https://www.census.gov/srd/csrm/SmallArea.html) of variables such as cropland area, building densities, settlement occurrences and woody vegetation cover that largely define the *anthropic* land cover types in a given country or in any other large geographical region of interest. We use the most recent GeoSurvey data and gridded covariates from Tanzania to illustrate the general approach and the key data analysis steps. These include sequentially:  

1. Data wrangling
2. Spatial prediction using ensemble machine learning
3. Small area estimation
4. Spatial monitoring and prediction

In order to monitor changing landscapes the first three steps should be repeated over time; i.e., to facilitate step 4. The end of this chapter gives some suggestions on as to how to do just that, though we cannot generate a fully-worked example of space-time land cover predictions at this point in time. 

## Data wrangling {#gs_step1}

Assembling analysis-ready dataframes is typically a major task that can take an inordinate amount of a data scientist's time and effort both inside and outside of the actual prediction workflows (e.g., for covariate image and GIS pre-processing and data vetting). Data providers are usually encouraged to provide *"clean"* datasets. While GeoSurvey does the *"clean / tidy bits"* pretty much automatically, in some instances the veracity of the observations, particularly from crowd-sourced data collections, should be thoroughly quality checked by GeoSurvey administrators. There is an example script for doing that at: [**Input screening**](https://github.com/mgwalsh/Geosurvey/blob/master/GS_input_screening.R).

We will use the following R packages to assemble a dataframe for Tanzania as an example of a typical data wrangling workflow:

```{r}
suppressPackageStartupMessages({
  require(downloader)
  require(rgdal)
  require(jsonlite)
  require(raster)
  require(leaflet)
  require(htmlwidgets)
  require(wordcloud)
})
```

### Load all the data locally {#local-load}

There are already many good examples of GeoSurveys of different countries that are available. For Africa, exemplar, *"tidied-up"* data are publicly available on the associated [**Open Science Framework**](https://osf.io/vxc97/) GeoSurvey repository, which also mirrors all of the associated data assembly and prediction code at [**Github**](https://github.com/mgwalsh/Cropland-Atlas). The following chunk loads all of the relevant data for Tanzania from its [**OSF repo**](https://osf.io/j8y3z/) and on Dropbox.
 
```{r}
dir.create("data", showWarnings = F)
dir.create("results", showWarnings = F)

# download GeoSurvey data
download("https://osf.io/uhx9b?raw=1", "data/TZ_geos_1718.csv.zip", mode = "wb")
unzip("data/TZ_geos_1718.csv.zip", overwrite = T)
geos <- read.table("data/TZ_geos_1718.csv", header = T, sep = ",")

# download GADM-L3 shapefile (courtesy: http://www.gadm.org)
download("https://www.dropbox.com/s/bhefsc8u120uqwp/TZA_adm3.zip?raw=1", "data/TZA_adm3.zip", mode = "wb")
unzip("data/TZA_adm3.zip", overwrite = T)
shape <- shapefile("data/TZA_adm3.shp")

# download raster stack (note this is a big 1+ Gb download)
download("https://osf.io/ke5ya?raw=1", "TZ_250m_2019.zip", mode = "wb")
unzip("TZ_250m_2019.zip", overwrite = T)
glist <- list.files(pattern="tif", full.names = T)
grids <- stack(glist)
```

### Spatial covariates (grids) {#spatial-covariates}

People ask what are all the spatial covariates that are used in the various predictions and where do they come from? There is no simple answer to this question as the variables keep changing as we obtain access to new open spatial data. However, the covariates often represent long-term average remote sensing measurements and/or other fairly slow geospatial variables, which are periodically updated as warranted. For Tanzania they currently include:

1. Climate related variables from e.g., [**CHELSA**](http://chelsa-climate.org/downloads/)
    - long-term mean temperature and precipitation 
    - mean annual temperature range
    - mean annual precipitation seasonality
2. Terrain data from e.g., [**MDEM**](http://hydro.iis.u-tokyo.ac.jp/~yamadai/MERIT_DEM/)
    - digital elevation
    - various terrain model derivatives including slope and compound topographic index.
3. Central place theory based variables from various sources
    - distances to major and minor roads from [**Geofabrik**](#https://www.geofabrik.de/data/download.html)
    - distances to known cities, towns and villages from [**Geofabrik**](#https://www.geofabrik.de/data/download.html)
    - distances to national, parks, wildlife conservation areas and forest reserves [**Protected Planet**](http://www.protectedplanet.net/)
    - distance to cell towers [**OpenCell**](https://unwiredlabs.com)
    - distance to main electrical grid
    - distance to open water sources [**Surface Water**](https://global-surface-water.appspot.com/)
4. Moderate resolution sensing data from various sources
    - long-term average Terra & Aqua MODIS data 
    - annual average Sentinel 1 & 2 data [**Copernicus**](http://lcviewer.vito.be/)
    - annual average Landsat-8 data [**USGS**](https://gisgeography.com/usgs-earth-explorer-download-free-landsat-imagery/)
    - long term average night lights [**NASA**](https://earthobservatory.nasa.gov/features/NightLights/page3.php)
5. Previous predictions of soil, settlement and land cover models e.g.,
    - ESA land cover fractions [**Copernicus**](http://lcviewer.vito.be/)
    - CIESIN/Facebook high resolution settlement data [**HRSL**](https://ciesin.columbia.edu/data/hrsl/)
    - ISRIC predictive soil maps [**SoilGrids**](https://soilgrids.org)
    - AfSIS prediction results from previous GeoSurveys [**AfSIS**](https://osf.io/vxc97/)
6. High-resolution remote sensing data where warranted and affordable e.g.,
    - from [**Planet Labs**](https://www.planet.com/)
    - from [**MAXAR**](https://www.digitalglobe.com/)
    
The current listing of the Tanzania grids that we are experimenting with is available in one of our short [**Tanzania reports 2019**](https://osf.io/7acs3/). All of the relevant layers were resampled to 250 m resolution on a common Lambert-Azimuthal Equal Area coordinate reference system `CRS(+proj=laea +ellps=WGS84 +lon_0=20 +lat_0=5 +units=m +no_defs")`. Note that the pre-processing of the resulting grid stack is generally done outside of R. We tend to use [**GRASS**](https://grass.osgeo.org/), because it is much faster and convenient to do it there, but there are many other GIS options which could be used for that purpose.

```{r Spatial_covariates, echo=FALSE, fig.cap="Examples of open-source spatial covariates used for making GeoSurvey predictions", fig.align="center", out.width = '90%'}
knitr::include_graphics("figures/spatial_covariate_examples.png")
```
  
### Assemble all of the data {#assemble-data}

The chunk below assembles the GeoSurvey and gridded data. It subsequently combines everything with the gridded variables in a consistent dataframe that can be used for spatial prediction.

```{r}
# attach GADM-L3 admin unit names from shape
coordinates(geos) <- ~lon+lat
projection(geos) <- projection(shape)
gadm <- geos %over% shape
geos <- as.data.frame(geos)
geos <- cbind(gadm[ ,c(5,7,9)], geos)
colnames(geos) <- c("region","district","ward","survey","time","id","observer","lat","lon","BP","CP","WP")

# project GeoSurvey coords to grid CRS
geos.proj <- as.data.frame(project(cbind(geos$lon, geos$lat), "+proj=laea +ellps=WGS84 +lon_0=20 +lat_0=5 +units=m +no_defs"))
colnames(geos.proj) <- c("x","y")
geos <- cbind(geos, geos.proj)
coordinates(geos) <- ~x+y
projection(geos) <- projection(grids)

# extract gridded variables at GeoSurvey locations
geosgrid <- extract(grids, geos)
gsdat <- as.data.frame(cbind(geos, geosgrid)) 
# gsdat <- gsdat[!duplicated(gsdat), ] ## removes any duplicates ... if needed
gsdat <- gsdat[complete.cases(gsdat[ ,c(10:13,19:67)]),] ## removes incomplete cases
gsdat$observer <- sub("@.*", "", as.character(gsdat$observer)) ## shortens observer ID's
```

### GeoSurvey dataframe {#dataframe}

The main end product of all of this wrangling (\@ref(geosurvey_step1) is a R dataframe called `gsdat` which contains the various GeoSurvey quadrat observations as well the collocated gridded measurements and their corresponding administrative boundaries from [**GADM**](https://gadm.org/data.html). You can look at the overall structure of this file with:

```{r}
str(gsdat)
```

Note that there are n = 16,219 (6.25 ha) quadrats (rows) and 69 variables (columns) in this dataframe. In this particular case, the actual GeoSurvey variables (sensu *features*) that we would like to predict from the spatial covariates include:

1. **BP**, the observed presence/absence of buildings in a quadrat
2. **CP**, the observed presence/absence of croplands in a quadrat
3. **WP**, the observed presence/absence of woody vegetation cover >60% in a quadrat

BP, CP, and WP predictions are used to generate a generalized Land Cover Classification (LCC), which focuses on the anthropic parts of landscapes that we subsequently use for survey post-stratification, small area estimation of cropland area, building densities and the development of ground-sampling plans e.g, for infrastructure surveys, soil and cropping systems and soil sampling plans that involve sending teams into the field. See the schema in (Fig. \@ref(fig:L3_schema) below.

```{r L3_schema, echo=FALSE, fig.cap="Land Cover Classification (LCC) schema", fig.align="center", out.width = '80%'}
knitr::include_graphics("figures/L3.png")
```

The following chunk writes the `gsdat` file (and potentially any others) out, which are subsequently used for the various spatial predictions. Writing the relevant files out simply means that one does not have to run the data wrangling step repeatedly, if the underlying data have not changed. It also produces simple Leaflet maps of potential sampling locations and/or tagged building coordinates (see below).

```{r}
write.csv(gsdat, "./results/TZ_gsdat_2018.csv", row.names = F)

# GeoSurvey map widget ----------------------------------------------------
w <- leaflet() %>%
  setView(lng = mean(gsdat$lon), lat = mean(gsdat$lat), zoom = 6) %>%
  addProviderTiles(providers$OpenStreetMap.Mapnik) %>%
  addCircleMarkers(gsdat$lon, gsdat$lat, clusterOptions = markerClusterOptions())
w ## plot widget 
```

Individual GeoSurvey contributions are acknowledged via a wordcloud, which highlights the proportion of the quadrats that were completed by each GeoSurveyor. This also essentially provides a specific survey's signature for label vetting and quality control purposes.

```{r}
# GeoSurvey contributions -------------------------------------------------
gscon <- as.data.frame(table(gsdat$observer))
set.seed(1235813)
wordcloud(gscon$Var1, freq = gscon$Freq, scale = c(4,0.1), random.order = T)
```

## Spatial prediction using ensemble machine learning {#geosurvey_step2}

We now turn to the prediction part of the GeoSurvey workflow. Many data scientists have their favorite algorithm(s) for predicting the types geospatial data that GeoSurvey produces. Also many new algorithms are being developed. The notion of there being potentially many *"favorite"*, new algorithms supports our notion that we might be able use some sort of a combination of those (favorites) to derive a weighted (*"ensemble"*) prediction at the end of the validation process. This is akin to using multiple experts in generating the predictions and then weighing those predictions based on their respective accuracies. Our schema for this is shown in Fig. \@ref(fig:GS_workflow).

```{r GS_workflow, echo=FALSE, fig.cap="Generalized GeoSurvey prediction workflow", fig.align="center", out.width = '90%'}
knitr::include_graphics("figures/geosurvey_prediction_workflow.png")
```

The overall prediction workflow shown in Fig. fig:\@ref(GS_workflow) is implemented in four main steps:

1. Label vetting and quality control
2. Model calibration and stacking
3. Ensemble prediction using multiple machine-learning algorithms
4. Reinforcement learning on prediction results from previous surveys.

The quality control and subsequent computational steps are intended to ensure that the results that are obtained are reproducible and have high predictive accuracies suitable for reinforcement learning as well as for monitoring changes. They are also intended to set up a test (or validation) set. The four steps are demonstrated by the code chunks provided hereafter.

### Label vetting and quality control {#label-vetting}

The following is an example where we had a commission [**Piece work**](https://en.wikipedia.org/wiki/Piece_work) from a consulting company to conduct a GeoSurvey of all of Africa (consisting of 1 million quadrat locations distributed over the photosynthetically active portion of Africa). The survey task involved tagging 1 km^2 pixels for both the presence/absence of buildings and croplands. This was one of the first crowd-sourcing surveys that we ran in 2015, and we were concerned about the accuracy of the resulting observations.

The main reason for our concern was that the students who were employed to do this survey, were not formally trained in air-photo and/or satellite interpretations, but also in part because the survey was done as *Piece work*, in which people get paid based the number of observations that they produce. The latter can result in people rushing through the observations and not paying adequate attention to their accuracy. You can download the cleaned data for this survey at its [**OSF repo**](https://osf.io/hmpw7/).

We decided to systematically check a sub-sample of the observations of the survey in proportion to the number of observations that had been registered by the participants in this survey. The checks were done by 2 GeoSurvey admins (*"image interpretation experts"*) who were both proficient in satellite image interpretations. The 2 admins had +60 years of experience between them at the time (... we believe in employing the elderly :). Their associated data and code chunks are presented below. The whole script can be forked from this [**Github repo**](https://github.com/mgwalsh/Geosurvey/blob/master/GS_input_screening.R). To document this more thoroughly, this initial chunk is the data download.

```{r}
library(downloader)
library(arm)

# Data download -----------------------------------------------------------
# Create a "Data" folder in your current working directory
dir.create("1MGS_data", showWarnings=F)
dat_dir <- "./1MGS_data"

# 1MQ GeoSurvey
download("https://www.dropbox.com/s/tnc1wwmi8a4h6b1/2015-01-20_18-00-02_248261.zip?raw=1", "./1MGS_data/2015-01-20_18-00-02_248261.zip", mode="wb")
unzip("./1MGS_data/2015-01-20_18-00-02_248261.zip", exdir="./1MGS_data", junkpaths=T, overwrite=T)
geosurvey_a1 <- read.csv(paste(dat_dir, "/6_1m-point-survey-a1.csv", sep=""), stringsAsFactors=F)
geosurvey_a2 <- read.csv(paste(dat_dir, "/7_1m-point-survey-a2.csv", sep=""), stringsAsFactors=F)
geosurvey_total <- rbind(geosurvey_a1[ ,1:6], geosurvey_a2[ ,1:6])

# Validation dataset
download("https://www.dropbox.com/s/pt86fr3ko379f8h/1MQ_validation_data.csv?raw=1", "./1MGS_data/1MQ_validation_data.csv", mode ="wb")
geosv <- read.csv(paste(dat_dir, "/1MQ_validation_data.csv", sep=""), stringsAsFactors=F)
geosv$User <- do.call("rbind", strsplit(geosv$User, split="@"))[,1]
```

This following chunk removes any quadrat duplicates, where those exist, by *accident* or *intent*.

```{r}
# Remove coordinate duplicates --------------------------------------------
dupindex <- which(duplicated(geosurvey_total[ ,2:3]))
dupindex_rev <- which(duplicated(geosurvey_total[ ,2:3], fromLast=T))
dupindex_total <- unique(c(dupindex, dupindex_rev))
geos_nodups <- geosurvey_total[-dupindex, ]
geos_nodups$User <- do.call("rbind", strsplit(geos_nodups$User, split="@"))[,1]
```

The main GeoSurveyor accuracy assessment chunk compares a statistical sample of what Geosurveyors (*users*) posted to what the corresponding *expert* assessments are. We also set some rules here, which state that if the level of agreement between *users* and *experts* is less than 50% then the user's classifications are not included in the final dataset.

```{r}
# Geosurveyor (GS) accuracy assessment ------------------------------------
# Cropland observations
geov_crp <- geosv[, c(1:4, 6, 8)]
# NA values in 1MQ are "Don't know", which should also be compared to expert answers to calculate accuracy rates
geov_crp[,5] <- ifelse(is.na(geov_crp[,5]), "NA", geov_crp[,5])
geov_crp[,6] <- ifelse(is.na(geov_crp[,6]), "NA", geov_crp[,6])

# Cropland random effects model, GS vs expert
CRPcn <- as.numeric(factor(geov_crp$CRPc))
CRPn <- as.numeric(factor(geov_crp$CRP))
CRP_diff <- ifelse(CRPcn - CRPn==0, 1, 0)
geosv_narm <- geov_crp[!is.na(CRP_diff), ]
CRP_diff <- na.omit(CRP_diff)
CRP.glmer <- glmer(CRP_diff~1+(1|geosv_narm$User), family=binomial)
display(CRP.glmer)
```

This chunk then just identifies users whose agreement rates are in <50% agreement with the experts and removes their data from further analyses.

```{r}
# If a GS's estimated agreement rate is <50%, the data for that GS are removed from further analyses
coef(CRP.glmer)
nosample_CRP <- rownames(coef(CRP.glmer)[[1]])[coef(CRP.glmer)[[1]][,1]<0]
crp_data <- geos_nodups
crp_data$Cropland.present <- ifelse(crp_data$User%in%nosample_CRP|crp_data$Cropland.present=="Don't Know", NA, crp_data$Cropland.present)
nrow(crp_data)

# Settlement observations
geov_hs <- geosv[, c(1:4, 5, 7)]
# NA values in 1MQ are "Don't know", which should also be compared to expert answers to calculate accuracy rates
geov_hs[,5] <- ifelse(is.na(geov_hs[,5]), "NA", geov_hs[,5])
geov_hs[,6] <- ifelse(is.na(geov_hs[,6]), "NA", geov_hs[,6])

# Settlement random effects model, GS vs expert
HSPcn <- as.numeric(factor(geov_hs$HSPc))
HSPn <- as.numeric(factor(geov_hs$HSP))
HSP_diff <- ifelse(HSPcn - HSPn==0, 1, 0)
geosv_narm <- geosv[!is.na(HSP_diff), ]
HSP_diff <- na.omit(HSP_diff)
HSP.glmer <- glmer(HSP_diff~1+(1|geosv_narm$User), family=binomial) ## mixed effects model
display(HSP.glmer)

# If a GS's estimated agreement rate is <50%, the data for that GS are removed from further analyses
coef(HSP.glmer)
nosample_HSP <- rownames(coef(HSP.glmer)[[1]])[coef(HSP.glmer)[[1]][,1]<0]
hsp_data <- geos_nodups
hsp_data[,5] <- ifelse(hsp_data$User%in%nosample_HSP|hsp_data[,5]=="Don't Know", NA, hsp_data[,5])
nrow(hsp_data)
```

In practice, we now monitor GeoSurveyor observations from within GeoSurvey itself in near-real-time. So the previous section is a bit of an aside. However, it does demonstrate how labeling might be controlled and adjusted statistically with a simple generalized linear [**Mixed-effects model**](https://en.wikipedia.org/wiki/Mixed_model).

### Model calibration {#calibration}

This section covers the code needed for model calibrations, stacking and validation procedures. To start training models, you will need to have the following R-packages installed.

```{r}
suppressPackageStartupMessages({
  require(devtools)
  require(caret)
  require(MASS)
  require(randomForest)
  require(gbm)
  require(nnet)
  require(plyr)
  require(doParallel)
  require(dismo)
})
```

If you have run the previous code, the needed data should be in memory and can be accessed via the subsequent chunk. Alternatively, you can also download the most recent `gsdat` file from its [**OSF repo**](https://osf.io/27avb/) to your local working directory.

```{r}
# Data setup --------------------------------------------------------------
rm(list=setdiff(ls(), c("gsdat","grids","glist"))) ## scrub extraneous objects in memory
# gsdat <- gsdat[complete.cases(gsdat[ ,c(17:49)]),] ## removes incomplete cases
```

We then split the `gsdat` file into 80:20% training (or calibration) and testing (or validation) sets. As the names imply the calibration data are used for calibrating the different models; whereas, the independent validation data are used to validate the predictions on any previously unseen data. This ensures that we can obtain reasonably unbiased accuracies of our predictions. It is also intended to guard against model [**Overfitting**](https://en.wikipedia.org/wiki/Overfitting), when using only cross-validation. Fig. \@ref(fig:training_validation) illustrates the relevant steps.

```{r training_validation, echo=FALSE, fig.cap="Model training and validation steps", fig.align="center", out.width = '90%'}
knitr::include_graphics("figures/training_validation.png")
```

The corresponding R code using the `caret` package is:

```{r}
# set calibration/validation set randomization seed
seed <- 12358
set.seed(seed)

# split data into calibration and validation sets
gsIndex <- createDataPartition(gsdat$BP, p = 4/5, list = F, times = 1)
gs_cal <- gsdat[ gsIndex,]
gs_val <- gsdat[-gsIndex,]

# GeoSurvey calibration labels
cp_cal <- gs_cal$CP

# raster calibration features
gf_cal <- gs_cal[,19:70]
```

We then calibrate various models. This first calibration chunk in the script illustrates the basic `caret` syntax using only a subset of the spatial covariates in a [**Generalized linear model**](https://en.wikipedia.org/wiki/Generalized_linear_model). We refer to this as a [**Central Place Theory Model**](https://en.wikipedia.org/wiki/Central_place_theory), in which central places of potential human impacts are identified with regard to their proximities to various infrastructure variables (e.g. roads, named cities, towns and villages, cell towers, electrical grid, various protected areas and places with access to surface water). 

```{r}
# Central place theory model <glm> -----------------------------------------
# select central place features / covariates
gf_cpv <- gs_cal[,28:39]

# start doParallel to parallelize model fitting
mc <- makeCluster(detectCores())
registerDoParallel(mc)

# control setup
set.seed(1385321)
tc <- trainControl(method = "cv", classProbs = T,
                   summaryFunction = twoClassSummary, allowParallel = T)

# model training
gl1 <- train(gf_cpv, cp_cal, 
             method = "glmStepAIC",
             family = "binomial",
             preProc = c("center","scale"), 
             trControl = tc,
             metric ="ROC")

# model outputs & predictions
summary(gl1)
print(gl1) ## ROC's accross cross-validation
gl1.pred <- predict(grids, gl1, type = "prob") ## spatial predictions

stopCluster(mc)
saveRDS(gl1, "./Results/gl1.rds")
```

Note that because calibration can be rather slow, the process is parallelized using the `doParallel` package. There are also other options available for this when using either hardwired *GPUs* and/or cloud computing resources such as [**AWS-EC2+**](https://aws.amazon.com/free/?sc_icampaign=acq_freetier&sc_icontent=awssm-evergreen-default-hero&sc_iplace=hero&trk=ha_awssm-evergreen-default-hero&sc_ichannel=ha&all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc) or [**Google Cloud**](https://cloud.google.com/).

```{r}
# GLM with all covariates -------------------------------------------------
# start doParallel to parallelize model fitting
mc <- makeCluster(detectCores())
registerDoParallel(mc)

# control setup
set.seed(1385321)
tc <- trainControl(method = "cv", classProbs = T,
                   summaryFunction = twoClassSummary, allowParallel = T)

# model training
gl2 <- train(gf_cal, cp_cal, 
             method = "glmStepAIC",
             family = "binomial",
             preProc = c("center","scale"), 
             trControl = tc,
             metric ="ROC")

# model outputs & predictions
summary(gl2)
print(gl2) ## ROC's accross cross-validation
gl2.pred <- predict(grids, gl2, type = "prob") ## spatial predictions

stopCluster(mc)
saveRDS(gl2, "./Results/gl2.rds")

# Random forest <randomForest> --------------------------------------------
# start doParallel to parallelize model fitting
mc <- makeCluster(detectCores())
registerDoParallel(mc)

# control setup
set.seed(1385321)
tc <- trainControl(method = "cv", classProbs = T,
                   summaryFunction = twoClassSummary, allowParallel = T)
tg <- expand.grid(mtry = seq(1,5, by=1)) ## model tuning steps

# model training
rf <- train(gf_cal, cp_cal,
            preProc = c("center","scale"),
            method = "rf",
            ntree = 501,
            metric = "ROC",
            tuneGrid = tg,
            trControl = tc)

# model outputs & predictions
print(rf) ## ROC's accross tuning parameters
# plot(varImp(rf)) ## relative variable importance
rf.pred <- predict(grids, rf, type = "prob") ## spatial predictions

stopCluster(mc)
saveRDS(rf, "./Results/rf.rds")

# Generalized boosting <gbm> ----------------------------------------------
# start doParallel to parallelize model fitting
mc <- makeCluster(detectCores())
registerDoParallel(mc)

# control setup
set.seed(1385321)
tc <- trainControl(method = "cv", classProbs = T, summaryFunction = twoClassSummary,
                   allowParallel = T)

## for initial <gbm> tuning guidelines see @ https://stats.stackexchange.com/questions/25748/what-are-some-useful-guidelines-for-gbm-parameters
tg <- expand.grid(interaction.depth = seq(2,5, by=1), shrinkage = 0.01, n.trees = seq(101,501, by=50),
                  n.minobsinnode = 50) ## model tuning steps

# model training
gb <- train(gf_cal, cp_cal, 
            method = "gbm", 
            preProc = c("center", "scale"),
            trControl = tc,
            tuneGrid = tg,
            metric = "ROC")

# model outputs & predictions
print(gb) ## ROC's accross tuning parameters
plot(varImp(gb)) ## relative variable importance
gb.pred <- predict(grids, gb, type = "prob") ## spatial predictions

stopCluster(mc)
saveRDS(gb, "./Results/gb.rds")

# Neural network <nnet> ---------------------------------------------------
# start doParallel to parallelize model fitting
mc <- makeCluster(detectCores())
registerDoParallel(mc)

# control setup
set.seed(1385321)
tc <- trainControl(method = "cv", classProbs = T,
                   summaryFunction = twoClassSummary, allowParallel = T)
tg <- expand.grid(size = seq(2,10, by=2), decay = c(0.001, 0.01, 0.1)) ## model tuning steps

# model training
nn <- train(gf_cal, cp_cal, 
            method = "nnet",
            preProc = c("center","scale"), 
            tuneGrid = tg,
            trControl = tc,
            metric ="ROC")

# model outputs & predictions
print(nn) ## ROC's accross tuning parameters
plot(varImp(nn)) ## relative variable importance
nn.pred <- predict(grids, nn, type = "prob") ## spatial predictions

stopCluster(mc)
saveRDS(nn, "./Results/nn.rds")
```

There are only a few limits on the number of currently available ML algorithms that could be tuned to the prediction task at hand. We restrict those to our favorites in this context (bagging, boosting & neural networks), but there is no reason why you can't or shouldn't explore others e.g., various Bayesian methods come to mind, though they tend to be computationally expensive.

### Model stacking and validation {#gs_stacking}

This section covers the general stacking process that we use. *"Stacking"* just means combining the various models that we ran during the calibration step into a single label prediction. This combined model is evaluated against previously unseen (validation) data to weight individual models and to ascertain accuracy against an independent validation set.

```{r}
# Model stacking setup ----------------------------------------------------
preds <- stack(1-gl1.pred, 1-gl2.pred, 1-rf.pred, 1-gb.pred, 1-nn.pred)
names(preds) <- c("gl1","gl2","rf","gb","nn")
plot(preds, axes = F)

# extract model predictions
coordinates(gs_val) <- ~x+y
projection(gs_val) <- projection(preds)
gspred <- extract(preds, gs_val)
gspred <- as.data.frame(cbind(gs_val, gspred))

# stacking model validation labels and features
cp_val <- gspred$CP ## change this to $BP, $WP ...
gf_val <- gspred[,71:75] ## subset validation features

# Model stacking ----------------------------------------------------------
# start doParallel to parallelize model fitting
mc <- makeCluster(detectCores())
registerDoParallel(mc)

# control setup
set.seed(1385321)
tc <- trainControl(method = "cv", classProbs = T, 
                   summaryFunction = twoClassSummary, allowParallel = T)

# model training
st <- train(gf_val, cp_val,
            method = "glm",
            family = "binomial",
            metric = "ROC",
            trControl = tc)

# model outputs & predictions
print(st)
plot(varImp(st))
st.pred <- predict(preds, st, type = "prob") ## spatial predictions
plot(1-st.pred, axes = F)

stopCluster(mc)
saveRDS(st, "./Results/st.rds")
```



```{r}
# Receiver-operator characteristics ---------------------------------------
cp_pre <- predict(st, gf_val, type="prob")
cp_val <- cbind(cp_val, cp_pre)
cpp <- subset(cp_val, cp_val=="Y", select=c(Y))
cpa <- subset(cp_val, cp_val=="N", select=c(Y))
cp_eval <- evaluate(p=cpp[,1], a=cpa[,1]) ## calculate ROC's on test set
plot(cp_eval, 'ROC') ## plot ROC curve

# Generate feature mask ---------------------------------------------------
t <- threshold(cp_eval) ## calculate thresholds based on ROC
r <- matrix(c(0, t[,1], 0, t[,1], 1, 1), ncol=3, byrow = T) ## set threshold value <kappa>
mask <- reclassify(1-st.pred, r) ## reclassify stacked predictions
plot(mask, axes=F)
```


```{r}
# Write prediction grids --------------------------------------------------
gspreds <- stack(preds, 1-st.pred, mask)
names(gspreds) <- c("gl1","gl2","rf","gb","nn","st","mk")
writeRaster(gspreds, filename="./Results/TZ_CP_preds_2018.tif", datatype="FLT4S", options="INTERLEAVE=BAND", overwrite=T)

# Write output data frame -------------------------------------------------
coordinates(gsdat) <- ~x+y
projection(gsdat) <- projection(grids)
gspre <- extract(gspreds, gsdat)
gsout <- as.data.frame(cbind(gsdat, gspre))
gsout$mzone <- ifelse(gsout$mk == 1, "Y", "N")
confusionMatrix(data = gsout$mzone, reference = gsout$CP, positive = "Y")
write.csv(gsout, "./Results/TZ_CP_out.csv", row.names = F) ## ... change the feature names here if needed

# Prediction map widget ---------------------------------------------------
pred <- 1-st.pred ## GeoSurvey ensemble probability
pal <- colorBin("Greens", domain = 0:1) ## set color palette
w <- leaflet() %>% 
  setView(lng = mean(gsdat$lon), lat = mean(gsdat$lat), zoom = 6) %>%
  addProviderTiles(providers$OpenStreetMap.Mapnik) %>%
  addRasterImage(pred, colors = pal, opacity = 0.4, maxBytes=6000000) %>%
  addLegend(pal = pal, values = values(pred), title = "Cropland prob.")
w ## plot widget 
saveWidget(w, 'TZ_CP_2018.html', selfcontained = T) ## save html ... change feature names here
```

## Small Area Estimates (SAE) {#gs_sae}

In this section we show how to use GeoSurvey data and predictions for small area estimation of cropland area proportions and building densities.

```{r}
# Required packages
# install.packages(c("downloader","rgdal","raster","MASS","arm",leaflet","htmlwidgets")), dependencies=TRUE)
suppressPackageStartupMessages({
  require(downloader)
  require(rgdal)
  require(raster)
  require(MASS)
  require(arm)
  require(leaflet)
  require(htmlwidgets)
})

# set working directory
dir.create("TZ_sae", showWarnings = F)
setwd("./TZ_sae")

# Data downloads -----------------------------------------------------------
# download GeoSurvey data
download("https://osf.io/27avb?raw=1", "TZ_gsdat_2018.csv.zip", mode = "wb")
unzip("TZ_gsdat_2018.csv.zip", overwrite = T)
geos <- read.table("TZ_gsdat_2018.csv", header = T, sep = ",")
vars <- c("region","district","ward","lat","lon","BP","CP","bcount","ccount","PH")
geos <- geos[vars] ## removes extraneous variables
geos <- geos[ which(geos$ccount < 17), ] ## drops cropland miscounts

# download GeoSurvey prediction rasters
download("https://osf.io/fdkz8?raw=1", "TZ_GS_preds.zip", mode = "wb")
unzip("TZ_GS_preds.zip", overwrite = T)
glist <- list.files(pattern="tif", full.names = T)
grids <- stack(glist)
# (gave <- cellStats(grids, mean)) ## calculates mean grids values

# project GeoSurvey coords to grid CRS
geos.proj <- as.data.frame(project(cbind(geos$lon, geos$lat), "+proj=laea +ellps=WGS84 +lon_0=20 +lat_0=5 +units=m +no_defs"))
colnames(geos.proj) <- c("x","y")
geos <- cbind(geos, geos.proj)
coordinates(geos) <- ~x+y
projection(geos) <- projection(grids)

# extract gridded variables at GeoSurvey locations
geosgrid <- extract(grids, geos)
saedat <- as.data.frame(cbind(geos, geosgrid)) 

# Write data frame --------------------------------------------------------
dir.create("Results", showWarnings = F)
write.csv(saedat, "./Results/TZ_sae.csv", row.names = F)
```

```{r}
# Cropland area models ----------------------------------------------------
# binomial models of GeoSurvey cropland grid counts
summary(m0 <- glm(cbind(ccount, 16-ccount) ~ 1, family=binomial, saedat)) ## mean model
(est0 <- cbind(Estimate = coef(m0), confint(m0))) ## standard 95% confidence intervals
# summary(mq <- glm(cbind(ccount, 16-ccount) ~ 1, family=quasibinomial(link="logit"), gsdat)) ## overdispersed model

# with cropland spatial presence prediction (CP18)
summary(m1 <- glm(cbind(ccount, 16-ccount) ~ CP18, family=binomial, saedat)) ## scaling model
(est1 <- cbind(Estimate = coef(m1), confint(m1))) ## 95% confidence intervals
m1.pred <- predict(grids, m1, type="response")
plot(m1.pred, axes=F)
# gsdat$m1 <- predict(m1, gsdat, type="response")

# with additional LCC covariates
summary(m2 <- glm(cbind(ccount, 16-ccount) ~ BP18*CP18*WP18, family=binomial, saedat))
(est2 <- cbind(Estimate = coef(m2), confint(m2))) ## standard 95% confidence intervals
anova(m1, m2) ## model comparison
m2.pred <- predict(grids, m2, type="response")
plot(m2.pred, axes=F)
# saedat$m2 <- predict(m2, saedat, type="response")

# Write prediction grids
gspreds <- stack(m1.pred, m2.pred)
names(gspreds) <- c("m1","m2")
writeRaster(gspreds, filename="./Results/TZ_cp_sae.tif", datatype="FLT4S", options="INTERLEAVE=BAND", overwrite=T)
```


```{r}
# Small area estimates (SAE)
# post-stratified by admin units
summary(m3 <- glmer(cbind(ccount, 16-ccount) ~ 1 + (1|region), family=binomial, saedat))

#  with additional LCC covariates
summary(m4 <- glmer(cbind(ccount, 16-ccount) ~ BP18*CP18*WP18 + (1|region), family=binomial, saedat))
ran <- ranef(m4) ## extract regional random effects
ses <- se.coef(m4) ## extract regional standard errors
nam <- rownames(ran$region)
sae <- as.data.frame(cbind(ran$region, ses$region)) ## regional-level small area estimates
colnames(sae) <- c("ran","se")
# par(pty="s", mar=c(10,10,1,1))
coefplot(ran$region[,1], ses$region[,1], varnames=nam, xlim=c(-1,1), CI=2, main="") ## region coefficient plot
write.csv(sae, "./Results/TZ_crop_area_sae.csv", row.names = F)

# Building count models ---------------------------------------------------
# Poisson models of GeoSurvey building counts
summary(m5 <- glm(bcount ~ 1, family=poisson, saedat)) ## country mean model
(est5 <- cbind(Estimate = coef(m5), confint(m5))) ## standard 95% confidence intervals
summary(mnb <- glm.nb(bcount ~ 1, saedat)) ## overdispersed negative binomial model
anova(m5, mnb)

# with building presence prediction (BP18, building probability)
summary(m6 <- glm(bcount ~ BP18, family=poisson, saedat)) ## scaling model
(est6 <- cbind(Estimate = coef(m6), confint(m6))) ## standard 95% confidence intervals
m6.pred <- predict(grids, m6, type="response")
plot(m6.pred, axes=F)
# gsdat$m6 <- predict(m6, gsdat, type="response")

# with additional LCC covariates
summary(m7 <- glm(bcount ~ BP18*CP18*WP18, family=poisson, saedat))
(est7 <- cbind(Estimate = coef(m7), confint(m7))) ## standard 95% confidence intervals
m7.pred <- predict(grids, m7, type="response")
plot(m7.pred, axes=F)
# saedat$m7 <- predict(m7, saedat, type="response")

# Write prediction grids
gspreds <- stack(m6.pred, m7.pred)
names(gspreds) <- c("m6","m7")
writeRaster(gspreds, filename="./Results/TZ_bcount_sae.tif", datatype="FLT4S", options="INTERLEAVE=BAND", overwrite=T)

# Small area estimates (SAE)
# post-stratified by admin units (30 regions)
summary(m8 <- glmer(bcount ~ 1 + (1|region), family=poisson, saedat))
ran <- ranef(m8) ## extract district random effects
ses <- se.coef(m8) ## extract district standard errors
nam <- rownames(ran$region)
sae <- as.data.frame(cbind(ran$region, ses$region)) ## region-level small area estimates
colnames(sae) <- c("ran","se")
coefplot(ran$region[,1], ses$region[,1], varnames=nam, xlim=c(-2,2), CI=2, main="") ## region coefficient plot

# with additional LCC covariates
summary(m9 <- glmer(bcount ~ BP18*CP18*WP18 + (1|region), family=poisson, saedat))
anova(m8, m9) ## model comparison
ran <- ranef(m9) ## extract region random effects
ses <- se.coef(m9) ## extract region standard errors
nam <- rownames(ran$region)
sae <- as.data.frame(cbind(ran$region, ses$region)) ## region-level small area estimates
colnames(sae) <- c("ran","se")
coefplot(ran$region[,1], ses$region[,1], varnames=nam, xlim=c(-1,1), CI=2, main="") ## region coefficient plot
write.csv(sae, "./Results/TZ_bcount_sae.csv", row.names = F)
```

