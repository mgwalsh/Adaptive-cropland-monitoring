# GeoSurvey prediction workflows {#geosurvey}

*Edited by: Markus Walsh, William Wu, Jiehua Chen, ... ?*

We gratefully acknowledge the team at [**QED**](https://qed.ai/), who built, maintain and continuously improve the [**GeoSurvey**](https://geosurvey.qed.ai/) App.

[**Dr. Alex Verlinden**](https://www.linkedin.com/in/alex-verlinden-1782a79/) (1956 - 2017) is gratefully acknowledged for his many contributions in developing GeoSurvey applications and for thoroughly training dozens of people in its use. 

## Introduction {#GS-intro}

Quantifying the geographical extent, location and spatial dynamics of cropland areas, rural and urban settlements and woody vegetation cover provides essential land cover information for monitoring and managing human dominated (*"anthropic"*) landscapes and the associated socio-economic, health, environmental and ecological impacts. Large portions of Africa remain *"terra incognita"* in this context. The main reason for gathering GeoSurvey observations is is that these allow us to rapidly assess where in a particular country significant impacts of humans on ecosystem processes can be expected. We have frequently been able to provide highly accurate assessments and spatial predictions of where anthropic impacts are likely to occur in remote places that are not readily reachable by car or even on foot.

[**GeoSurvey**](https://geosurvey.qed.ai/) is a platform for analyzing geospatial land cover observations. High resolution satellite images, mobile phone photography, and drone imagery can be systematically labeled by either trained photo interpreters and/or by *"crowds"* of [**citizen scientists**](https://en.wikipedia.org/wiki/Citizen_science). When done with care, these observations result in large, well-structured, properly labeled, geospatial data sets that are suitable for machine learning and geostatistical predictions of land cover and in some instances for predicting land use. The detailed manual for conducting your own GeoSurveys is available at: [**GeoSurvey manual**](https://docs.google.com/document/d/1y-HYUSYpDVESPdmEcl3I2kuL0bwrT41wMiq0zE9uzOs/edit). The manual should definitely be consulted to obtain information about how GeoSurvey can be used to carry out potentially high value surveys. There is also a great slide deck available [**here**](https://docs.google.com/presentation/d/1vBQ-By8LLvyJQzMBFaqUuRwFFeL7Y8QXUtBifx-3jn4/edit#slide=id.g14d47405c8_0_0), which illustrates different land cover and use labeling approaches. We shall not cover these issues in this chapter and assume that we already have well-designed GeoSurvey data and collocated spatial covariates in hand.

```{r GS_screenshot, echo=FALSE, fig.cap="Screenshot of the [**GeoSurvey App**](https://geosurvey.qed.ai/) in action over a quadrat in Rwanda", fig.align="center", out.width = '90%'}
knitr::include_graphics("figures/geosurvey_screenshot.png")
```

This chapter provides an overview of ensemble machine learning and statistical workflows, which we have used for mapping and interpreting GeoSurvey land cover observations from Digital Globe (Worlview etc) satellite images. We recognize that there are potentially many different techniques and algorithms that could be applied in this context. Hence, the workflows presented here are completely open for constructive criticism and improvement. They could (and probably should) also be exposed and stress-tested in data science competitions e.g., at [**Kaggle**](https://www.kaggle.com/), given their potential importance for monitoring and managing terra incognita landscapes.

The intent of the chapter is to provide starter code for predictive mapping and statistical small area estimation [**SAE**](https://www.census.gov/srd/csrm/SmallArea.html) of variables such as cropland area, building densities, settlement occurrences and woody vegetation cover that largely define the *anthropic* land cover types in a given country or in any other large geographical region of interest. We use the most recent GeoSurvey data and gridded covariates from Tanzania to illustrate the general approach and the key data analysis steps. These include sequentially:  

1. Data wrangling
2. Spatial prediction using ensemble machine learning
3. Small area estimation
4. Spatial monitoring and prediction

In order to monitor changing landscapes the first three steps should be repeated over time; i.e., to facilitate step 4. The end of this chapter gives some suggestions on as to how to do just that, though we cannot generate a fully-worked example of space-time land cover predictions at this point in time. 

## Step 1: Data wrangling

Assembling analysis-ready dataframes is typically a major task that can take an inordinate amount of a data scientist's time and effort both inside and outside of the actual prediction workflows (e.g., for covariate image and GIS pre-processing and data vetting). Data providers are encouraged to provide *"clean"* datasets. While GeoSurvey does the *"clean bits"* pretty much automatically, in some instances the veracity of the observations, particularly from crowd-sourced data collections, should be thoroughly quality checked by GeoSurvey administrators. There is an example script for doing that at: [**Input screening**](https://github.com/mgwalsh/Geosurvey/blob/master/GS_input_screening.R).

We will use the following R packages to assemble a dataframe for Tanzania as an example of a typical data wrangling workflow:

```{r}
suppressPackageStartupMessages({
  require(downloader)
  require(rgdal)
  require(jsonlite)
  require(raster)
  require(leaflet)
  require(htmlwidgets)
  require(wordcloud)
})
```

### Load all the data locally {#local-load}

There are already many good examples of GeoSurveys of different countries that are available. For Africa, exemplar, *"tidied-up"* data are publicly available on the associated [**Open Science Framework**](https://osf.io/vxc97/) GeoSurvey repository, which also mirrors all of the associated data assembly and prediction code at [**Github**](https://github.com/mgwalsh/Cropland-Atlas).

The following chunk loads all of the relevant data for Tanzania from its [**OSF repo**](https://osf.io/j8y3z/) and Dropbox:
 
```{r}
dir.create("figures", showWarnings = F)
dir.create("data", showWarnings = F)
dir.create("results", showWarnings = F)

# download GeoSurvey data from OSF or Dropbox
# also see the corresponding sampling frame @ https://github.com/mgwalsh/Sampling/blob/master/TZ_GS_sample.R
download("https://osf.io/9gndf?raw=1", "./data/TZ_geos_2018.csv.zip", mode = "wb")
unzip("./data/TZ_geos_2018.csv.zip", overwrite = T)
geos <- read.table("./data/TZ_geos_2018.csv", header = T, sep = ",")
geos$BIC <- as.factor(ifelse(geos$CP == "Y" & geos$BP == "Y", "Y", "N")) ## identifies croplands with buildings

# download GADM-L3 shapefile (courtesy: http://www.gadm.org)
download("https://www.dropbox.com/s/bhefsc8u120uqwp/TZA_adm3.zip?raw=1", "./data/TZA_adm3.zip", mode = "wb")
unzip("./data/TZA_adm3.zip", overwrite = T)
shape <- shapefile("./data/TZA_adm3.shp")

# download raster stack (note this is a big 1+ Gb download, which a take a while to download depending ...)
# download("https://osf.io/ke5ya?raw=1", "TZ_250m_2019.zip", mode = "wb") ## OSF is persistent
# download("https://www.dropbox.com/s/ejl3h62hojnhh3a/TZ_250m_2019.zip?raw=1", "TZ_250m_2019.zip", mode = "wb") ## DB tends to be faster
unzip("TZ_250m_2019.zip", overwrite = T)
glist <- list.files(pattern="tif", full.names = T)
grids <- stack(glist)
grids ## examines the structure of the grid stack and its dimensions
```

### Spatial covariates (grids) {#spatial-covariates}

People ask what are all the spatial covariates that are used in the various predictions and where do they come from? These keep changing these as we get access to new open spatial data. However, the covariates often represent long-term averages and/or fairly slow geospatial variables, which are periodically updated as warranted. For Tanzania these currently include:

1. Climate related variables from e.g., [**CHELSA**](http://chelsa-climate.org/downloads/)
    - long-term mean temperature and precipitation 
    - mean annual temperature range
    - mean annual precipitation seasonality
2. Terrain data from e.g., [**MDEM**](http://hydro.iis.u-tokyo.ac.jp/~yamadai/MERIT_DEM/)
    - digital elevation
    - various terrain model derivatives including slope and compound topographic index.
3. Central place theory based variables from various sources
    - distances to major and minor roads from [**Geofabrik**](#https://www.geofabrik.de/data/download.html)
    - distances to known cities, towns and villages from [**Geofabrik**](#https://www.geofabrik.de/data/download.html)
    - distances to national, parks, wildlife conservation areas and forest reserves [**Protected Planet**](http://www.protectedplanet.net/)
    - distance to cell towers [**OpenCell**](https://unwiredlabs.com)
    - distance to main electrical grid
    - distance to open water sources [**Surface Water**](https://global-surface-water.appspot.com/)
4. Moderate resolution sensing data from various sources
    - long-term average Terra & Aqua MODIS data 
    - annual average Sentinel 1 & 2 data [**Copernicus**](http://lcviewer.vito.be/)
    - annual average Landsat-8 data [**USGS**](https://gisgeography.com/usgs-earth-explorer-download-free-landsat-imagery/)
    - long term average night lights [**NASA**](https://earthobservatory.nasa.gov/features/NightLights/page3.php)
5. Previous predictions of soil, settlement and land cover models e.g.,
    - ESA land cover fractions [**Copernicus**](http://lcviewer.vito.be/)
    - CIESIN/Facebook high resolution settlement data [**HRSL**](https://ciesin.columbia.edu/data/hrsl/)
    - ISRIC predictive soil maps [**SoilGrids**](https://soilgrids.org)
    - AfSIS prediction results from previous GeoSurveys [**AfSIS**](https://osf.io/vxc97/)
6. High-resolution remote sensing data where warranted and affordable e.g.,
    - from [**Planet Labs**](https://www.planet.com/)
    - from [**MAXAR**](https://www.digitalglobe.com/)
    
The full current listing of the Tanzania grids that we are experimenting with is available in one of the [**Tanzania reports 2019**](https://osf.io/7acs3/). All of the layers were resampled to 250 m resolution on a common Lambert-Azimuthal Equal Area coordinate reference system `CRS(+proj=laea +ellps=WGS84 +lon_0=20 +lat_0=5 +units=m +no_defs")`. Note that the pre-processing of the resulting grid stack is generally done outside of R. We use [**GRASS**](https://grass.osgeo.org/), because it is much faster and convenient to do it there, but there are many other GIS which could be used for this purpose.
  
### Assemble all of the data {#geosurvey_step1}

The chunk below assembles the GeoSurvey and gridded data. It includes total building counts and cropland proportion estimates based on a grid count (both per 6.25 ha quadrat), and it subsequently combines everything with the gridded variables in a consistent dataframe that can be used for spatial prediction.

```{r}
# attach GADM-L3 admin unit names from shape
coordinates(geos) <- ~lon+lat
projection(geos) <- projection(shape)
gadm <- geos %over% shape
geos <- as.data.frame(geos)
geos <- cbind(gadm[ ,c(5,7,9)], geos)
colnames(geos) <- c("region","district","ward","survey","time","id","observer","lat","lon","BP","CP","WP","rice","bloc","cgrid","BIC")

# Coordinates and number of buildings per quadrat -------------------------
bp <- geos[which(geos$BP == "Y"), ] ## identify quadrats with buildings
bp$bloc <- as.character(bp$bloc)

# coordinates of tagged building locations from quadrats with buildings
c <- fromJSON(bp$bloc[1])
bcoord <- do.call("rbind", c$feature$geometry$coordinates)
for(i in 2:nrow(bp)) {
  c <- fromJSON(bp$bloc[i])
  bcoord_temp <- do.call("rbind", c$feature$geometry$coordinates)
  bcoord <- rbind(bcoord, bcoord_temp)
}
bcoord <- as.data.frame(bcoord) ## vector of coordinates per quadrats with buildings
colnames(bcoord) <- c("lon","lat")

# number of tagged building locations from quadrats with buildings
bcount <- rep(NA, nrow(bp))
for(i in 1:nrow(bp)) {
  t <- fromJSON(bp$bloc[i])
  bcount[i] <- nrow(t$features)
}
ba <- geos[which(geos$BP == "N"), ]
ba$bcount <- 0
bp <- cbind(bp, bcount)
geos <- rbind(ba, bp)
geos <- geos[order(geos$id),] ## sort in original sample order

# cropland grid count
cp <- geos[which(geos$CP == "Y"), ] ## identify quadrats with cropland
cp$cgrid <- as.character(cp$cgrid)

# number of tagged grid locations from quadrats with cropland
ccount <- rep(NA, nrow(cp))
for(i in 1:nrow(cp)) {
  t <- fromJSON(cp$cgrid[i])
  ccount[i] <- nrow(t$features)
}
ca <- geos[which(geos$CP == "N"), ]
ca$ccount <- 0
cp <- cbind(cp, ccount)
geos <- rbind(ca, cp)
geos <- geos[order(geos$id),] ## sort in original sample order

# project GeoSurvey coords to grid CRS
geos.proj <- as.data.frame(project(cbind(geos$lon, geos$lat), "+proj=laea +ellps=WGS84 +lon_0=20 +lat_0=5 +units=m +no_defs"))
colnames(geos.proj) <- c("x","y")
geos <- cbind(geos, geos.proj)
coordinates(geos) <- ~x+y
projection(geos) <- projection(grids)

# extract gridded variables at GeoSurvey locations
geosgrid <- extract(grids, geos)
gsdat <- as.data.frame(cbind(geos, geosgrid)) 
# gsdat <- gsdat[!duplicated(gsdat), ] ## removes any duplicates ... if needed
gsdat <- gsdat[complete.cases(gsdat[ ,c(10:13,19:67)]),] ## removes incomplete cases
gsdat$observer <- sub("@.*", "", as.character(gsdat$observer)) ## shortens observer ID's
```

### GeoSurvey dataframe 

The main end product of all of this wrangling (\@ref(geosurvey_step1) is a R dataframe called `gsdat` which contains the various GeoSurvey quadrat observations as well the colocated gridded measurements and their corresponding administrative boundaries from [**GADM**](https://gadm.org/data.html). You can look at the overall structure of this file with:

```{r}
str(gsdat)
```

Note that there are n = 16,219 (6.25 ha) quadrats (rows) and 69 variables (columns) in this dataframe. In this particular case, the actual GeoSurvey variables that we would like to predict from the spatial covariates include:

1. **BP**, the observed presence/absence of buildings in a quadrat
2. **CP**, the observed presence/absence of croplands in a quadrat
3. **WP**, the observed presence/absence of woody vegetation cover > 60% in a quadrat
4. **rice**, the presence/absence of paddy rice in a quadrat
5. **bcount**, a count of all tagged buildings in a quadrat
6. **ccount**, a grid count of the proportion of cropland area (n/16) in a quadrat

BP, CP, and WP predictions are used to generate a Land Cover Classification (LCC) focused on the anthropic parts of landscapes, which we use for survey post-stratification, small area estimation of cropland area, building densities and development of ground-sampling plans e.g, for infrastructure surveys, soil and cropping systems and soil sampling plans that involve sending teams into the field. See the schema (Fig. \@ref(fig:L3_schema) below.

```{r L3_schema, echo=FALSE, fig.cap="Land Cover Classification (LCC) schema", fig.align="center", out.width = '80%'}
knitr::include_graphics("figures/L3.png")
```

The following chunk writes the `gsdat` file and others out, which are subsequently used for the various spatial predictions. Writing the relevant files out simply means that one does not have to run the data wrangling step repeatedly if the underlying data have not changed. It also produces simple Leaflet maps of potential sampling locations and/or tagged building coordinates (see below).

```{r}
write.csv(gsdat, "./results/TZ_gsdat_2018.csv", row.names = F)

# GeoSurvey map widget ----------------------------------------------------
w <- leaflet() %>%
  setView(lng = mean(gsdat$lon), lat = mean(gsdat$lat), zoom = 6) %>%
  addProviderTiles(providers$OpenStreetMap.Mapnik) %>%
  addCircleMarkers(gsdat$lon, gsdat$lat, clusterOptions = markerClusterOptions())
```

```{r echo = FALSE}
w ## plot widget 
```

Individual GeoSurvey contributions are acknowledged via a simple wordcloud, which highlights the proportion of the quadrats that were completed by each GeoSurveyor. This also essentially provides a survey's signature for label vetting and quality control purposes.

```{r}
# GeoSurvey contributions -------------------------------------------------
gscon <- as.data.frame(table(gsdat$observer))
set.seed(1235813)
wordcloud(gscon$Var1, freq = gscon$Freq, scale = c(4,0.1), random.order = T)
```

## Step 2: Spatial prediction using ensemble machine learning {#geosurvey_step2}

We now turn to the prediction part of the workflow. Many data scientists have their favorite algorithm(s) for predicting the types geospatial data that GeoSurvey produces. Also many new algorithms are being developed. The notion of there being potentially many *"favorite"*, new algorithms supports our notion that we might be able use some sort of a combination of those (favorites) to derive a weighted (*"ensemble"*) prediction at the end of the process. This is akin to using multiple experts in generating the predictions and then weighing those predictions based on their respective accuraccies. Our schema for this is shown in the following Figure:

```{r GS_workflow, echo=FALSE, fig.cap="Generalized GeoSurvey prediction workflow", fig.align="center", out.width = '90%'}
knitr::include_graphics("figures/geosurvey_prediction_workflow.png")
```

The overall prediction workflow shown in Fig. fig:\@ref(GS_workflow) is implemented in four main steps:

1. Label vetting and quality control
2. Model calibration and stacking
3. Ensemble prediction using multiple machine-learning algorithms
4. Reinforcement learning on prediction results from previous surveys.

The quality control and subsequent computational steps are intended to ensure that the results obtained are reproducible and have high predictive accuracies suitable for reinforcement learning as well as for monitoring changes. They are also intended to set up a validation set. The four steps are demonstrated by the code chunks provided hereafter.

### Label vetting and quality control {#label-vetting}

The following is an example where we had commissioned [**Piece work**](https://en.wikipedia.org/wiki/Piece_work) from a consulting company to conduct a GeoSurvey of all of Africa (consisting of 1 million quadrat locations distributed over the photosynthetically active portion of Africa). The survey task involved tagging 1 km^2 pixels for both the presence/absence of buildings and croplands. This was one of the first crowd-sourcing surveys that we ran in 2015, and we were concerned about the accuracy of the resulting observations.

The main reason for our concern was that the students who were employed to do this survey were not formally trained in air-photo and/or satellite interpretations, but also because the survey was done as *Piece work*, in which people get paid based the number of observations that they produce. The latter can result in people rushing through the observations and not paying adequate attention to their accuracy. You can download the cleaned data for this survey at its [**OSF repo**](https://osf.io/hmpw7/).

We decided to systematically check a sub-sample of the observations of the survey in proportion to the number of observations that had been registered by the participants in this survey. The checks were done by 2 GeoSurvey admins (*"image interpretation experts"*) who were both proficient in satellite image interpretions. The 2 admins had +60 years of experience between them at the time (... we believe in employing the elderly :). Their associated data and code chunks are presented below. The whole script can be downloaded at this [**Github repo**](). To document this more thoroughly, this initial chunk is the data download:

```{r}
library(downloader)
library(arm)

# Data download -----------------------------------------------------------
# Create a "Data" folder in your current working directory
dir.create("1MGS_data", showWarnings=F)
dat_dir <- "./1MGS_data"

# 1MQ GeoSurvey
download("https://www.dropbox.com/s/tnc1wwmi8a4h6b1/2015-01-20_18-00-02_248261.zip?raw=1", "./1MGS_data/2015-01-20_18-00-02_248261.zip", mode="wb")
unzip("./1MGS_data/2015-01-20_18-00-02_248261.zip", exdir="./1MGS_data", junkpaths=T, overwrite=T)
geosurvey_a1 <- read.csv(paste(dat_dir, "/6_1m-point-survey-a1.csv", sep=""), stringsAsFactors=F)
geosurvey_a2 <- read.csv(paste(dat_dir, "/7_1m-point-survey-a2.csv", sep=""), stringsAsFactors=F)
geosurvey_total <- rbind(geosurvey_a1[ ,1:6], geosurvey_a2[ ,1:6])

# Validation dataset
download("https://www.dropbox.com/s/pt86fr3ko379f8h/1MQ_validation_data.csv?raw=1", "./1MGS_data/1MQ_validation_data.csv", mode ="wb")
geosv <- read.csv(paste(dat_dir, "/1MQ_validation_data.csv", sep=""), stringsAsFactors=F)
geosv$User <- do.call("rbind", strsplit(geosv$User, split="@"))[,1]
```

This following chunk removes any coordinate duplicates, where those exist ... by *"accident"*.

```{r}
# Remove coordinate duplicates --------------------------------------------
dupindex <- which(duplicated(geosurvey_total[ ,2:3]))
dupindex_rev <- which(duplicated(geosurvey_total[ ,2:3], fromLast=T))
dupindex_total <- unique(c(dupindex, dupindex_rev))
geos_nodups <- geosurvey_total[-dupindex, ]
geos_nodups$User <- do.call("rbind", strsplit(geos_nodups$User, split="@"))[,1]
```

The following is the main GeoSurveyor accuracy assessment chunck, which compares a statistical sample of what Geosurveyors (*users*) post, to what the *expert* assessments are. We also set some rules here, which state that if the level of agreement between *users* and *experts* is less than 50% then the user's classifications are not included in the final dataset.

```{r}
# Geosurveyor (GS) accuracy assessment ------------------------------------
# Cropland observations
geov_crp <- geosv[, c(1:4, 6, 8)]
# NA values in 1MQ are "Don't know", which should also be compared to expert answers to calculate accuracy rates
geov_crp[,5] <- ifelse(is.na(geov_crp[,5]), "NA", geov_crp[,5])
geov_crp[,6] <- ifelse(is.na(geov_crp[,6]), "NA", geov_crp[,6])

# Cropland random effects model, GS vs expert
CRPcn <- as.numeric(factor(geov_crp$CRPc))
CRPn <- as.numeric(factor(geov_crp$CRP))
CRP_diff <- ifelse(CRPcn - CRPn==0, 1, 0)
geosv_narm <- geov_crp[!is.na(CRP_diff), ]
CRP_diff <- na.omit(CRP_diff)
CRP.glmer <- glmer(CRP_diff~1+(1|geosv_narm$User), family=binomial)
display(CRP.glmer)

# If a GS's estimated agreement rate is <50%, the data for that GS are removed from further analyses
coef(CRP.glmer)
nosample_CRP <- rownames(coef(CRP.glmer)[[1]])[coef(CRP.glmer)[[1]][,1]<0]
crp_data <- geos_nodups
crp_data$Cropland.present <- ifelse(crp_data$User%in%nosample_CRP|crp_data$Cropland.present=="Don't Know", NA, crp_data$Cropland.present)
nrow(crp_data)

# Building / rural settlement observations
geov_hs <- geosv[, c(1:4, 5, 7)]
# NA values in 1MQ are "Don't know", which should also be compared to expert answers to calculate accuracy rates
geov_hs[,5] <- ifelse(is.na(geov_hs[,5]), "NA", geov_hs[,5])
geov_hs[,6] <- ifelse(is.na(geov_hs[,6]), "NA", geov_hs[,6])

# Building / rural settlement random effects model, GS vs expert
HSPcn <- as.numeric(factor(geov_hs$HSPc))
HSPn <- as.numeric(factor(geov_hs$HSP))
HSP_diff <- ifelse(HSPcn - HSPn==0, 1, 0)
geosv_narm <- geosv[!is.na(HSP_diff), ]
HSP_diff <- na.omit(HSP_diff)
HSP.glmer <- glmer(HSP_diff~1+(1|geosv_narm$User), family=binomial)
display(HSP.glmer)

# If a GS's estimated agreement rate is <50%, the data for that GS are removed from further analyses
coef(HSP.glmer)
nosample_HSP <- rownames(coef(HSP.glmer)[[1]])[coef(HSP.glmer)[[1]][,1]<0]
hsp_data <- geos_nodups
hsp_data[,5] <- ifelse(hsp_data$User%in%nosample_HSP|hsp_data[,5]=="Don't Know", NA, hsp_data[,5])
nrow(hsp_data)
```

### Model calibration and stacking {#calibration-and-stacking}

